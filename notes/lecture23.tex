\section{Newton's method}
Up until now, we have only considered first order  methods to optimize functions. Now, we will utilize second order information to achieve a faster rate of convergence.

As always, our objective is to minimize a function $f\colon \R^n \rightarrow \R$. The basic idea of Newton's Method is to set the first-order Taylor expansion of the gradient to zero: $F(x) = \nabla f(x) = 0$. This leads to an iterative update step that will (under certain conditions) lead to a significantly faster convergence rate than gradient descent methods.

To illustrate the point, consider a single variable function $\varphi\colon\R \rightarrow \R$. Our goal is to solve the non-linear equation $\varphi(x) = 0$. From Taylor's theorem, we can express the first-order form of $\varphi(x)$ as
$$\varphi(x) = \varphi(x_0) + \varphi'(x) \cdot (x-x_0) + o(\abs{x-x_0})$$
given $\delta = x - x_0$ we equivalently have that
$$\varphi(x_0 + \delta) = \varphi(x_0) + \varphi'(x) \cdot \delta + o(\abs{\delta})$$
Disregarding the $o(\abs{\delta})$ term, we solve (over $\delta$) the following objective: $$\varphi(x_0) + \varphi'(x_0) \delta = 0$$
Then, $\delta = -\frac{\varphi(x_0)}{\varphi'(x_0)}$, leading to the iteration 
$x_{t+1} = x_t - \frac{\varphi(x_t)}{\varphi'(x_t)}$.

We can similarly make an argument for a multi variable function $F: \R^d \rightarrow \R$. Our goal is to solve $F(x) = 0$. Again, from Taylor's theorem we have that
$$F(x + \Delta) = F(x) +  J_F(x) \Delta + o(\| \Delta\|)$$
where $J_F$ is the Jacobian. This gives us
$\Delta = -J_F^{-1}(x)F(x)$, and the iteration
$$x_{t+1} = x_t - J_F^{-1}(x_t)F(x_t)$$

Given $f\colon \R \rightarrow \R$, Newton's method applies this update to $F(x) = \nabla f(x) = 0$. It uses the update rule
$$x_{t+1} = x_t - \nabla^2 f(x_t)^{-1} \nabla f(x_t)$$
A Newton step minimizes the second order Taylor approximation
$$f(x) \approx f(x_t) + \nabla f(x_t) ^\trans (x - x_t) + \frac{1}{2} (x - x_t) ^\trans \nabla^2 f(x_t) (x - x_t)$$

Now, we will show that Newton's method converges to a local minimum, 
given a starting point that is within a neighborhood of that point.
\begin{theorem}
Given $f: \R^n \rightarrow \R$ and assuming that
\begin{enumerate}
\item $f$ is twice continuously differentiable
\item $\nabla^2 f(x)$ is Lipschitz: $\| \nabla^2 f(x) - \nabla^2 f(x') \| \leq \|x-x'\|$
\item $\exists x^*$ s.t. $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*) \succeq \alpha I$ and $\|x^0 - x^*\| \leq \frac{\alpha}{2}$
\end{enumerate}
Then, $\|x_{t+1} - x^*\| \leq \frac{1}{\alpha} \|x_t - x^*\|^2$
\end{theorem}

\begin{proof}
Given that $\nabla f(x^*) = 0$, we have that
\begin{align*}
x_{t+1} - x^* &= x_t - x^* - \nabla^2 f(x_t)^{-1} \nabla f(x_t) \\
&= \nabla^2 f(x_t)^{-1} [\nabla^2 f(x_t)(x_t - x^*) - (\nabla f(x_t) - \nabla f(x^*))]
\end{align*}
This implies that
\[
\|x_{t+1} - x^* \| \leq 
\|\nabla^2 f(x_t)^{-1} \| \cdot 
\|\nabla^2 f(x_t)(x_t - x^*) - (\nabla f(x_t) - \nabla f(x^*))\|
\]
%
\begin{claim}
$\|\nabla^2 f(x_t)(x_t - x^*) - (\nabla f(x_t) - \nabla f(x^*))\|
\le \frac{1}{2} \| x^t - x^* \|^2$
\end{claim}
\begin{proof}
Applying the integral remainder form of Taylor's theorem to $\nabla f(x_t)$ we have that
$$\nabla f(x_t) - \nabla f(x^*) = \int_{0}^{1} \nabla^2 f(x_t + \gamma (x^* - x_t)) \cdot (x_t - x^*) d\gamma$$
We therefore have that
\begin{align*}
&\|\nabla^2 f(x_t)(x_t - x^*) - (\nabla f(x_t) - \nabla f(x^*))\| \\
&= \|\int_{0}^{1} [\nabla^2 f(x_t) - \nabla^2 f(x_t + \gamma (x^* - x_t))](x_t - x^*) d\gamma \| \\
&\leq \int_{0}^{1} \|\nabla^2 f(x_t) - \nabla^2 f(x_t + \gamma (x^* - x_t))\| \cdot \|x_t - x^*\| d\gamma \\
&\leq \left( \int_{0}^{1} \gamma d\gamma \right) \|x_t - x^*\|^2 \tag{$\nabla^2 f(x_t)$ is Lipschitz}\\
&= \frac{1}{2}\|x_t - x^*\|^2
\end{align*}
\end{proof}

\begin{claim}
$\|\nabla^2 f(x_t)^{-1}\| \leq \frac{2}{\alpha} $ 
\end{claim}
\begin{proof}
By the Wielandt-Hoffman Theorem,
\begin{align*}
\abs{\lambda_{min}(\nabla^2 f(x_t)) - \lambda_{min}(\nabla^2 f(x^*))} &\leq \|\nabla^2 f(x_t) - \nabla^2 f(x^*) \| \\
&\leq \|x_t - x^*\| \tag{$\nabla^2 f(x_t)$ is Lipschitz}
\end{align*}
Thus, for $\|x_t - x^*\| \leq \frac{\alpha}{2}$ and given that $\nabla^2 f(x^*) \succeq \alpha I$, this implies that $\lambda_{min}(\nabla^2 f(x_t)) \geq \frac{\alpha}{2}$. Hence, $\| \nabla^2 f(x_t)^{-1} \| \leq \frac{2}{\alpha}$.
\end{proof}
Putting the two claims together, we have that
$$\|x_{t+1} - x^*\| \leq \frac{2}{\alpha} \cdot \frac{1}{2} \|x_t - x^*\|^2 = \frac{1}{\alpha} \|x_t - x^*\|^2$$
\end{proof}
Note that we did not need convexity in the proof. Given that we are within a neighborhood of the local minimum $x^*$, then we can achieve $\epsilon$ error in just $O(\log \log \frac{1}{\epsilon})$ iterations. (this is called \emph{quadratic convergence}.)

\subsection{Damped update}
In general, Newton's method can be quite unpredictable.
For example, consider the function 
\[
f(x) = \sqrt{x^2 +1}\,
\] 
essentially a smoothed version of the absolute value $|x|$.
Clearly, the function is minimized at $x^* = 0$. 
Calculating the necessary derivatives for Newton's method, we find
\begin{align*}
    f^\prime (x) &= \dfrac{x}{\sqrt{x^2 + 1}}\, \\
    f^{\prime \prime} (x) &= (1+x^2)^{-3/2}\,.
\end{align*}
Note that $f(x)$ is strongly convex since its second derivative strictly positive 
and $1$-smooth ($|f^\prime (x)| < 1$). 
The Newton step for minimizing $f(x)$ is
\begin{align*}
    x_{t+1} = x_t - \dfrac{f^\prime (x_t)}{f^{\prime \prime}(x)} = -x_t^3\,.
\end{align*}
The behavior of this algorithm depends on the magnitude of $x_t$. 
In particular, we have the following three regimes
\[ \begin{cases} 
      |x_t| < 1 & \text{Algorithm converges \textit{cubically}}\\
      |x_t| = 1 & \text{Algorithm oscillates between $-1$ and $1$} \\
      |x_t| > 1 & \text{Algorithm diverges} 
   \end{cases}
\]
This example shows that even for strongly convex functions with Lipschitz gradients that Newton's method is only guaranteed to converge locally. To avoid divergence, a popular technique is to use a \textit{damped} step--size:
\begin{align*}
    x_{t+1} = x_t - \eta_t \nabla^2 f(x_t)^{-1} \nabla f(x_t)
\end{align*}
$\eta_t$ can be chosen by backtracking line search. Usually though $\eta = 1$ is a good first choice since, if you are in a region of convergence, you are guaranteed quadratic convergence.

\subsection{Quasi-Newton methods}
Let's compare gradient descent and Newton's method side by side.
    \begin{align*}
        x_{t+1} &= x_t - \eta_t \nabla f(x_t) \tag{Gradient descent}\\
        x_{t+1} &= x_t - \nabla^2 f(x_t)^{-1} \nabla f(x_t) \tag{Newton's method}
    \end{align*}
We can think of gradient descent as a Newton update in which we approximate
$\nabla^2 f(x_t)^{-1}$ by a scaled version of the identity. 
That is, gradient descent is equivalent to Newton's method when $\nabla^2
f(x_t)^{-1} = \eta_t I$ where $I$ is the identity matrix. 

Quasi-Newton methods take 
the analogy a step further by approximating the Hessian by some other matrix.
The idea in doing so is to avoid an expensive matrix inversion at each step.
What we want is an approximation
\begin{align*}
    \hat{f}_{B_t}(x) \approx f(x_t) + \nabla f(x_t)^\trans (x - x_t) + \dfrac{1}{2} (x-x_t)B_t^{-1} (x-x_t)
\end{align*}
such that:
\begin{enumerate}
    \item $\nabla \hat{f}_{B_t} (x_t) = \nabla f(x_t)$.\\
    It seems reasonable that our approximation should be the same up to first order. 
    \item $\nabla \hat{f}_{B_t} (x_{t-1}) = \nabla f(x_{t-1})$\\
    This condition states that the gradient should still be correct at the previous iterate.
\end{enumerate}
 If the two last gradients are correct, we can expect our Hessian approximation to be reasonable along the direction $x_t - x_{t-1}$. This is called a \emph{secant approximation} which can be written as 
 \begin{align*}
     \nabla \hat{f}_{B_t} (x_{t+1}) = \nabla f(x_t) - B_t^{-1}(x_{t+1} - x_t)
 \end{align*}
 If we let
 \begin{align*}
     s_t &= x_{t+1} - x_t \\
     y_t &= \nabla \hat{f}_{B_t}(x_{t+1}) - \nabla f(x_t)
 \end{align*}
 Then we arrive at the \emph{Secant Equation}
 \begin{align*}
     s_t = B_{t} y_t
 \end{align*}
 There could be multiple $B_t$ that satisfy this condition. We can enforce other constraints to help narrow down on a particular choice. Some popular requirements are requiring $B_t$ to be positive definite, making sure $B_t$ is as close to $B_{t-1}$ as possible for some appropriate metric, or requiring $B_t$ to be a low--rank update of previous iterates where the update can be done via the Sherman--Morrison formula.  One of the most successful implementations of this is called BFGS named after Broyden, Fletcher, Goldfarb, Shanno and its limited--memory counterpart, L--BFGS.
