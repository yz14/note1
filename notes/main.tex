\documentclass[12pt]{article}

\usepackage{macros}

\input{title}

\begin{document}

\maketitle

\input{abstract}

\pagebreak

\setcounter{tocdepth}{2}
\tableofcontents

\pagebreak

\input{contributors}

\pagebreak
\part{Gradient methods}
\label{part:basic}
\input{gradient_intro}

\input{lecture01}
\input{lecture02}
\input{lecture03}
\input{lecture04}
\input{lecture05}

\pagebreak
\part{Accelerated gradient methods}
\label{part:accelerated}

We will now develop a set of techniques that allows us to obtain faster rates of
convergence than those of the basic gradient method. In the case of quadratics,
the ideas are simple, natural, and lead to practically important algorithms. The
theme of \emph{acceleration} extends to arbitrary smooth and convex functions,
even if the resulting method is not necessarily superior in practice.  Ending on
a note of caution, we will see how acceleration trades off with
\emph{robustness}. Accelerated gradient methods inherently lack the robustness
to noise that the basic gradient method enjoys.

\input{lecture06}
\input{lecture07}
\input{lecture08}
\input{lecture09}

\pagebreak
\part{Stochastic optimization}
\label{part:stochastic}
\input{lecture10}
\input{lecture11}
\input{lecture12}

\pagebreak
\part{Dual methods}
\label{part:dual}
\input{lecture13}
\input{lecture14}
\input{lecture15}
\input{lecture16}

\pagebreak
\part{Non-convex problems}
\label{part:nonconvex}
\input{lecture17}
\input{lecture18}
\input{lecture19}
\input{lecture20}
\input{lecture21}

\pagebreak
\part{Higher-order and interior point methods}
\label{part:higher}
\input{lecture23}
\input{lecture24}
\input{lecture25}
\input{lecture26}

\bibliographystyle{alpha}
\bibliography{notes}

\end{document}
